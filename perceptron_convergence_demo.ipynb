{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9239db30",
   "metadata": {},
   "source": [
    "# Perceptron Convergence Theorem — Numerical Illustration\n",
    "\n",
    "This notebook illustrates Rosenblatt's perceptron convergence theorem in a simple two–dimensional setting.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Construct a **linearly separable** dataset and train a perceptron.\n",
    "2. Track the quantities that appear in the classical convergence proof:\n",
    "   - The inner product between the current weight vector $w_t$ and a fixed separating vector $w^*$.\n",
    "   - The norm $\\lVert w_t \\rVert$.\n",
    "3. Compare the empirical number of updates with the theoretical bound $ (R/\\gamma)^2 $, where\n",
    "   - $R = \\max_i \\lVert x_i \\rVert$,\n",
    "   - $\\gamma = \\min_i y_i (w^* \\cdot x_i + b^*)$ is a margin for a separating hyperplane $(w^*, b^*)$.\n",
    "4. Show a simple **non-separable** example (XOR-type dataset) where the perceptron does **not** converge (it keeps making mistakes if we let it run forever).\n",
    "\n",
    "---\n",
    "\n",
    "> **Important assumption in the theorem:**  \n",
    "> The perceptron convergence theorem only guarantees convergence if there *exists* a hyperplane that strictly separates the two classes.  \n",
    "> A generic random dataset in $\\mathbb{R}^d$ will not necessarily be linearly separable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619bee6",
   "metadata": {},
   "source": [
    "## 1. Linearly separable data\n",
    "\n",
    "We generate synthetic data using a *hidden* separating hyperplane $(w^*, b^*)$ with a positive margin.\n",
    "We then sample points and label them by the sign of $w^* \\cdot x + b^*$, rejecting points that lie too close\n",
    "to the boundary to keep a reasonably large margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_separable_data(n_samples=60, dim=2, margin=0.05, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    w_star = rng.randn(dim)\n",
    "    w_star = w_star / np.linalg.norm(w_star)\n",
    "    b_star = rng.randn() * 0.1  # small random bias\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < n_samples:\n",
    "        x = rng.randn(dim)\n",
    "        m = w_star @ x + b_star\n",
    "        if abs(m) < margin:\n",
    "            # too close to the boundary, skip to maintain a margin\n",
    "            continue\n",
    "        label = 1 if m > 0 else -1\n",
    "        X.append(x)\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y, w_star, b_star\n",
    "\n",
    "X, y, w_star, b_star = generate_separable_data()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Number of +1 labels:\", np.sum(y == 1))\n",
    "print(\"Number of -1 labels:\", np.sum(y == -1))\n",
    "\n",
    "# Quick scatter plot of the data\n",
    "plt.figure()\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"+1\")\n",
    "plt.scatter(X[y == -1, 0], X[y == -1, 1], label=\"-1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.title(\"Linearly separable data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7d394",
   "metadata": {},
   "source": [
    "## 2. Perceptron algorithm and tracking of proof quantities\n",
    "\n",
    "We implement the standard perceptron update rule for labels $y_i \\in \\{-1, +1\\}$:\n",
    "\n",
    "$$\n",
    "\\text{If } y_i (w \\cdot x_i + b) \\le 0,\\quad\n",
    "w \\leftarrow w + y_i x_i, \\quad b \\leftarrow b + y_i.\n",
    "$$\n",
    "\n",
    "During training we track, after each *update*:\n",
    "\n",
    "- $w_t \\cdot w^*$\n",
    "- $\\lVert w_t \\rVert^2$\n",
    "\n",
    "and we also count the total number of updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_train(X, y, max_epochs=1000, w_star=None, b_star=None):\n",
    "    n_samples, dim = X.shape\n",
    "    w = np.zeros(dim)\n",
    "    b = 0.0\n",
    "\n",
    "    history = {\n",
    "        \"w\": [],\n",
    "        \"b\": [],\n",
    "        \"w_dot_wstar\": [],\n",
    "        \"w_norm_sq\": [],\n",
    "        \"mistakes_per_epoch\": []\n",
    "    }\n",
    "\n",
    "    total_updates = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        mistakes = 0\n",
    "        for i in range(n_samples):\n",
    "            if y[i] * (w @ X[i] + b) <= 0:\n",
    "                # misclassified, perform update\n",
    "                w = w + y[i] * X[i]\n",
    "                b = b + y[i]\n",
    "                mistakes += 1\n",
    "                total_updates += 1\n",
    "\n",
    "                if w_star is not None:\n",
    "                    history[\"w_dot_wstar\"].append(w @ w_star)\n",
    "                else:\n",
    "                    history[\"w_dot_wstar\"].append(None)\n",
    "                history[\"w_norm_sq\"].append(np.dot(w, w))\n",
    "\n",
    "                history[\"w\"].append(w.copy())\n",
    "                history[\"b\"].append(b)\n",
    "\n",
    "        history[\"mistakes_per_epoch\"].append(mistakes)\n",
    "\n",
    "        if mistakes == 0:\n",
    "            break\n",
    "\n",
    "    return w, b, history, total_updates\n",
    "\n",
    "w, b, history, total_updates = perceptron_train(X, y, max_epochs=100, w_star=w_star, b_star=b_star)\n",
    "\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)\n",
    "print(\"Total number of updates:\", total_updates)\n",
    "print(\"Number of epochs run:\", len(history['mistakes_per_epoch']))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"mistakes_per_epoch\"], marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Number of mistakes\")\n",
    "plt.title(\"Mistakes per epoch (separable case)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1647703",
   "metadata": {},
   "source": [
    "## 3. Visualizing the learned decision boundary\n",
    "\n",
    "In $\\mathbb{R}^2$ the perceptron defines a line\n",
    "\n",
    "$$\n",
    "\\{x \\in \\mathbb{R}^2 : w \\cdot x + b = 0\\}.\n",
    "$$\n",
    "\n",
    "We plot this line together with the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96219fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, b, X, y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"+1\")\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], label=\"-1\")\n",
    "\n",
    "    if w[1] != 0:\n",
    "        x_vals = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)\n",
    "        y_vals = -(w[0] * x_vals + b) / w[1]\n",
    "        plt.plot(x_vals, y_vals, label=\"perceptron boundary\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Perceptron decision boundary\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(w, b, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bd87d",
   "metadata": {},
   "source": [
    "## 4. Connecting to the convergence proof\n",
    "\n",
    "The classical proof assumes that there exists a separating hyperplane $(w^*, b^*)$ such that\n",
    "\n",
    "$$\n",
    "y_i (w^* \\cdot x_i + b^*) \\ge \\gamma > 0 \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "and that\n",
    "\n",
    "$$\n",
    "\\lVert x_i \\rVert \\le R \\quad \\text{for all } i.\n",
    "$$\n",
    "\n",
    "It then shows that the perceptron makes at most $ (R/\\gamma)^2 $ updates.\n",
    "\n",
    "Here we estimate empirical values of $R$ and $\\gamma$ using our *hidden* separating hyperplane $(w^*, b^*)$\n",
    "and compare this bound with the observed number of updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f119b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R and gamma for our synthetic data and the \"teacher\" hyperplane (w_star, b_star)\n",
    "norms = np.linalg.norm(X, axis=1)\n",
    "R = norms.max()\n",
    "margins = y * (X @ w_star + b_star)\n",
    "gamma = margins.min()\n",
    "\n",
    "print(\"R (max ||x_i||):\", R)\n",
    "print(\"gamma (margin):\", gamma)\n",
    "if gamma > 0:\n",
    "    bound = (R / gamma) ** 2\n",
    "    print(\"Theoretical upper bound on number of updates (R/gamma)^2:\", bound)\n",
    "else:\n",
    "    print(\"Warning: gamma <= 0, data not strictly separable by (w*, b*).\")\n",
    "\n",
    "print(\"Observed number of updates:\", total_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9f400",
   "metadata": {},
   "source": [
    "### Evolution of $w_t \\cdot w^*$ and $\\lVert w_t \\rVert^2$\n",
    "\n",
    "A key part of the proof is to show that:\n",
    "\n",
    "1. $w_{t+1} \\cdot w^* \\ge w_t \\cdot w^* + \\gamma$ (it grows at least linearly with the number of mistakes), and\n",
    "2. $\\lVert w_{t+1} \\rVert^2 \\le \\lVert w_t \\rVert^2 + R^2$ (it grows at most linearly with the number of mistakes).\n",
    "\n",
    "Together, these imply a contradiction if the number of updates exceeds $ (R/\\gamma)^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dot_wstar = np.array([v for v in history[\"w_dot_wstar\"] if v is not None])\n",
    "w_norm_sq = np.array(history[\"w_norm_sq\"])\n",
    "\n",
    "steps = np.arange(1, len(w_norm_sq) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, w_dot_wstar)\n",
    "plt.xlabel(\"Update step t\")\n",
    "plt.ylabel(\"w_t · w*\")\n",
    "plt.title(\"Growth of w_t · w*\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, w_norm_sq)\n",
    "plt.xlabel(\"Update step t\")\n",
    "plt.ylabel(\"||w_t||^2\")\n",
    "plt.title(\"Growth of ||w_t||^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e5e70",
   "metadata": {},
   "source": [
    "## 5. Non-separable data (XOR example)\n",
    "\n",
    "To emphasize that the convergence theorem **requires** linear separability, we now construct a simple non-separable\n",
    "dataset in $\\mathbb{R}^2$: an XOR pattern.\n",
    "\n",
    "No hyperplane can separate the positive and negative points here, and the perceptron will keep making mistakes\n",
    "indefinitely (we just stop it after a fixed number of epochs).\n",
    "\n",
    "XOR-like dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da985d-ab8a-4ccc-8ceb-539c418be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.array([[1, 1],\n",
    "                  [1, -1],\n",
    "                  [-1, 1],\n",
    "                  [-1, -1]], dtype=float)\n",
    "y_xor = np.array([1, -1, -1, 1])  # XOR labels\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], label=\"+1\")\n",
    "plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], label=\"-1\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"XOR: not linearly separable\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab904037",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xor, b_xor, history_xor, total_updates_xor = perceptron_train(X_xor, y_xor, max_epochs=50)\n",
    "\n",
    "print(\"Total updates (non-separable, capped at 50 epochs):\", total_updates_xor)\n",
    "print(\"Mistakes per epoch:\", history_xor[\"mistakes_per_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(w_xor, b_xor, X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3a1f7",
   "metadata": {},
   "source": [
    "You should see that the perceptron continues to make mistakes in many epochs and does not find\n",
    "a perfect separating hyperplane — consistent with the fact that no such hyperplane exists for the XOR dataset.\n",
    "\n",
    "This illustrates the key point of Rosenblatt's perceptron convergence theorem:\n",
    "\n",
    "> **Convergence is guaranteed only under the assumption of strict linear separability.**\n",
    "\n",
    "For a generic random dataset, this assumption will *not* automatically hold, so the perceptron may fail to converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
